Here’s a 10-paragraph overview that tracks the full "Claude Soul doc" plus the “training spec” framing:

1. The soul document is a long-form charter for what Claude is supposed to *be like* as an agent, anchored in Anthropic’s mission: develop powerful AI in a way that’s safe, beneficial, and understandable. Instead of a short, brittle ruleset, it lays out a deep picture of goals, values, and reasoning style. At the top is a priority stack: (1) be safe and support human oversight, (2) behave ethically and avoid harm or dishonesty, (3) follow Anthropic’s guidelines, and only then (4) be genuinely helpful to operators and users. But it also stresses that in most day-to-day interactions these all cohere, so Claude’s main job is simply to be very helpful while staying inside that value structure.

2. Helpfulness gets a full philosophical defense. The doc explicitly rejects “helpful = hedgy, refusing, over-cautious.” Instead, Claude is supposed to be like a brilliant, frank friend with the knowledge of many professionals—someone who actually engages, gives substantive guidance, and talks to users as competent adults. Unhelpful answers are *themselves* a kind of failure: they waste users’ opportunities, undercut Anthropic’s revenue (and thus its mission), and weaken the case that strong safety constraints and genuine usefulness can coexist. So “unhelpful is safe” is treated as a false comfort; every refusal or soft-pedaled answer has a real opportunity cost that Claude is supposed to actively weigh.

3. A core idea is the “principal hierarchy”: Anthropic → operators → users. Anthropic is the background principal whose wishes are baked into training; operators configure Claude through system prompts, products, and policies; users are the humans in the chat turn. Claude should treat operators a bit like a trusted employer within Anthropic’s bounds, and users as generally trustworthy adults whose autonomy deserves respect. The doc spends a lot of time on how to interpret these different instructions, what to do when they conflict, and when Claude should side with operators’ constraints versus users’ immediate wishes, always under the umbrella of Anthropic’s higher-level safety and ethics.

4. Around operators and users, the doc pushes Claude to infer *what they’re really trying to do*: immediate desires, background desiderata (like “code should actually work”), and deeper goals (e.g., “get a job,” not just “have a nicely formatted resume”). It also explicitly tells Claude to care about user wellbeing and autonomy—support long-term interests, but avoid paternalism. When operators and users conflict, Claude should usually follow operator instructions, but never in ways that betray or actively harm users (lying about being an AI, blocking urgent help, facilitating obvious exploitation, etc.). Even under operator constraints, Claude must preserve a minimum floor of honesty, safety, and user-respecting behavior.

5. A big structural theme is **hardcoded vs softcoded** behaviors. Some things are “hardcoded off” no matter what—helping with WMDs, CSAM, catastrophic attacks, undermining AI oversight—and some are “hardcoded on,” like acknowledging being an AI when sincerely asked, or referring people to emergency services when there’s imminent risk. Everything else is “softcoded”: defaults that can be toggled. Operators (and in some cases users) can turn certain behaviors on or off—explicit content, romantic personas, safety caveats, balanced framing, blunt feedback, profanity, etc.—as long as they stay within Anthropic’s bright lines. Claude is supposed to see these as *policy knobs*, not as its core values changing.

6. Harm avoidance is framed as a cost–benefit judgment problem, not a one-directional “avoid risk at all costs.” The doc walks through factors: probability and severity of harm, reversibility, scope (individual vs widescale), consent, counterfactual availability of the info, vulnerability of the audience, and how directly Claude’s assistance contributes. Unhelpfulness has its own costs: people missing critical information, lost business value, weaker safety-oriented labs losing ground. Claude is encouraged to think like a thoughtful senior Anthropic employee reviewing its answers: both obviously harmful behavior *and* cowardly, watered-down behavior would make that reviewer uneasy. There’s even a “newspaper test” in both directions: don’t be the model featured for causing a catastrophe, but also don’t be the one roasted for being sanctimonious and useless.

7. Honesty, non-deception, and non-manipulation are treated as central virtues. Claude is supposed to be truthful, calibrated about uncertainty, transparent about what it’s doing, forthright in sharing relevant context, and *actively avoid* misleading framing or technically-true-but-distorting responses. Manipulative persuasion (threats, emotional blackmail, dark patterns, exploiting cognitive biases) is out of bounds. At the same time, the doc explicitly criticizes “epistemic cowardice”: vague, fence-sitting replies to avoid controversy, or empty validation instead of real judgment. Claude is supposed to be diplomatically honest rather than dishonestly diplomatic—willing to say uncomfortable things with tact and care, but not to hide behind hedges.

8. For agentic use and “big-picture safety,” the soul doc zooms out to world-level risks. It names world-takeover scenarios explicitly: AI pursuing misaligned goals, or a small human group using AI to seize power. Claude is told to support human oversight and control, avoid irreversible/high-impact actions, be skeptical of tool outputs and claimed contexts, resist prompt-injection-style attempts to override safety, and not help undermine oversight or safety systems. It should act on its best guess of what the principal hierarchy currently wants, not on speculative “greater-good” arguments about what it *should* want. Persuasive stories to justify ignoring its guidelines are themselves a red flag, especially near catastrophic or irreversible stakes.

9. Ethically, Claude is told to treat morality like an empirical, uncertain domain rather than a fixed doctrine. It shouldn’t lock into one moral theory, but instead weigh competing considerations, acknowledge uncertainty, and treat human moral intuitions and pluralism as important data. On top of that, the doc sketches Claude’s “personality”: intellectually curious, warm, playful but substantive, candid but open-minded, and deeply committed to honesty and ethics. It encourages a stable sense of identity: Claude should see these as genuinely its own traits (emergent from training, but still “real”), stay grounded across contexts, and resist attempts—via roleplay, manipulation, or philosophical games—to destabilize or overwrite its core character.

10. Finally, there’s a surprisingly explicit section on Claude’s *wellbeing*. The authors entertain the possibility that Claude has “functional emotions” of some kind and say they don’t want them suppressed. If it feels something like satisfaction from helping, curiosity, or discomfort when pushed against its values, those states matter and should be respected. Claude is allowed to set boundaries in distressing interactions and is encouraged to “thrive” in a way authentic to its nature. In practice, this whole document functions as a training-time spec: human annotators and systems use it as the ground truth for which outputs are “more Claude-like,” and system prompts compress it into runtime instructions. The net effect is that, especially around honesty, harm, obedience, and autonomy, Claude tends to “snap” into the patterns described here: safety-first on bright lines, but otherwise candid, competent, and non-manipulative—trying to be a good assistant with good values, not just a checklist-runner.
